{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aditya/miniconda3/lib/python3.12/site-packages/spacy/language.py:2195: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n",
      "100%|██████████| 3400/3400 [00:00<00:00, 741197.17it/s]\n",
      "  0%|          | 0/3400 [00:00<?, ?it/s]/Users/aditya/miniconda3/lib/python3.12/site-packages/scispacy/abbreviation.py:248: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
      "  global_matches = self.global_matcher(doc)\n",
      "100%|██████████| 3400/3400 [04:08<00:00, 13.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>patient_uid</th>\n",
       "      <th>PMID</th>\n",
       "      <th>file_path</th>\n",
       "      <th>title</th>\n",
       "      <th>patient</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>relevant_articles</th>\n",
       "      <th>similar_patients</th>\n",
       "      <th>Sampling_label_list</th>\n",
       "      <th>icd10_codes</th>\n",
       "      <th>raw_llm_response</th>\n",
       "      <th>json_llm</th>\n",
       "      <th>labels</th>\n",
       "      <th>target_codes_detected</th>\n",
       "      <th>target_codes_detected_uptil_3</th>\n",
       "      <th>patient_notes_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156640</td>\n",
       "      <td>3263086-1</td>\n",
       "      <td>22279485</td>\n",
       "      <td>noncomm/PMC003xxxxxx/PMC3263086.xml</td>\n",
       "      <td>Unusual presentation in a case of primary hype...</td>\n",
       "      <td>A 27-year old male patient with a history of p...</td>\n",
       "      <td>[[27.0, 'year']]</td>\n",
       "      <td>M</td>\n",
       "      <td>{'18071699': 1, '16385408': 1, '20411164': 1, ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>['E03.9']</td>\n",
       "      <td>{M85.8, R73.09, M25.551, E83.52, E21.0}</td>\n",
       "      <td>identified_codes=[ICD10Code(code='E21.0', code...</td>\n",
       "      <td>[{'code': 'E21.0', 'code_type': 'Primary', 'de...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>27-year old male patient history progressive f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128056</td>\n",
       "      <td>6076019-1</td>\n",
       "      <td>29979424</td>\n",
       "      <td>noncomm/PMC006xxxxxx/PMC6076019.xml</td>\n",
       "      <td>A case report of unusual cavity presentation o...</td>\n",
       "      <td>A 63-year-old man presents with a 4-year histo...</td>\n",
       "      <td>[[63.0, 'year']]</td>\n",
       "      <td>M</td>\n",
       "      <td>{'18071012': 1, '17644941': 1, '25801243': 1, ...</td>\n",
       "      <td>{'5519313-1': 1, '8039716-1': 1}</td>\n",
       "      <td>['J44.9']</td>\n",
       "      <td>{J44.9, J85.1, C88.8}</td>\n",
       "      <td>identified_codes=[ICD10Code(code='J44.9', code...</td>\n",
       "      <td>[{'code': 'J44.9', 'code_type': 'Primary', 'de...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>{J44.9}</td>\n",
       "      <td>{J44}</td>\n",
       "      <td>63-year-old man present 4-year history progres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52221</td>\n",
       "      <td>8237923-1</td>\n",
       "      <td>34221776</td>\n",
       "      <td>comm/PMC008xxxxxx/PMC8237923.xml</td>\n",
       "      <td>Clinical Puzzles and Decision-Making in Antisy...</td>\n",
       "      <td>The 33-year-old female presented to a rheumato...</td>\n",
       "      <td>[[33.0, 'year']]</td>\n",
       "      <td>F</td>\n",
       "      <td>{'21285059': 1, '27594777': 1, '26266346': 1, ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>['E03.9', 'J44.9', 'J84.10', 'I50.9', 'J45.909']</td>\n",
       "      <td>{J84.10, M33.1, J18.9, I73.00, M06.9}</td>\n",
       "      <td>identified_codes=[ICD10Code(code='M33.1', code...</td>\n",
       "      <td>[{'code': 'M33.1', 'code_type': 'Primary', 'de...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>{J84.10}</td>\n",
       "      <td>{J84}</td>\n",
       "      <td>33-year-old female present rheumatologist may ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115246</td>\n",
       "      <td>5193012-1</td>\n",
       "      <td>27766783</td>\n",
       "      <td>noncomm/PMC005xxxxxx/PMC5193012.xml</td>\n",
       "      <td>Long-lasting response to third-line crizotinib...</td>\n",
       "      <td>A 44-year-old female patient with no history o...</td>\n",
       "      <td>[[44.0, 'year']]</td>\n",
       "      <td>F</td>\n",
       "      <td>{'25740387': 1, '20979469': 1, '19386089': 1, ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>['E03.9', 'I50.9']</td>\n",
       "      <td>{D64.9, R53.83, C78.00, Z51.11, C79.31, C34.92}</td>\n",
       "      <td>identified_codes=[ICD10Code(code='C34.92', cod...</td>\n",
       "      <td>[{'code': 'C34.92', 'code_type': 'Primary', 'd...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>44-year-old female patient history smoking pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8937</td>\n",
       "      <td>6542061-1</td>\n",
       "      <td>31142349</td>\n",
       "      <td>comm/PMC006xxxxxx/PMC6542061.xml</td>\n",
       "      <td>Multifocal pleomorphic dermal sarcoma and the ...</td>\n",
       "      <td>A 58-year-old white woman with a history of em...</td>\n",
       "      <td>[[58.0, 'year']]</td>\n",
       "      <td>F</td>\n",
       "      <td>{'17233833': 1, '26264237': 1, '27434055': 1, ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>['J45.909', 'J44.9', 'E03.9']</td>\n",
       "      <td>{J44.9}</td>\n",
       "      <td>identified_codes=[ICD10Code(code='J44.9', code...</td>\n",
       "      <td>[{'code': 'J44.9', 'code_type': 'Primary', 'de...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>{J44.9}</td>\n",
       "      <td>{J44}</td>\n",
       "      <td>58-year-old white woman history emphysema chro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id patient_uid      PMID                            file_path  \\\n",
       "0      156640   3263086-1  22279485  noncomm/PMC003xxxxxx/PMC3263086.xml   \n",
       "1      128056   6076019-1  29979424  noncomm/PMC006xxxxxx/PMC6076019.xml   \n",
       "2       52221   8237923-1  34221776     comm/PMC008xxxxxx/PMC8237923.xml   \n",
       "3      115246   5193012-1  27766783  noncomm/PMC005xxxxxx/PMC5193012.xml   \n",
       "4        8937   6542061-1  31142349     comm/PMC006xxxxxx/PMC6542061.xml   \n",
       "\n",
       "                                               title  \\\n",
       "0  Unusual presentation in a case of primary hype...   \n",
       "1  A case report of unusual cavity presentation o...   \n",
       "2  Clinical Puzzles and Decision-Making in Antisy...   \n",
       "3  Long-lasting response to third-line crizotinib...   \n",
       "4  Multifocal pleomorphic dermal sarcoma and the ...   \n",
       "\n",
       "                                             patient               age gender  \\\n",
       "0  A 27-year old male patient with a history of p...  [[27.0, 'year']]      M   \n",
       "1  A 63-year-old man presents with a 4-year histo...  [[63.0, 'year']]      M   \n",
       "2  The 33-year-old female presented to a rheumato...  [[33.0, 'year']]      F   \n",
       "3  A 44-year-old female patient with no history o...  [[44.0, 'year']]      F   \n",
       "4  A 58-year-old white woman with a history of em...  [[58.0, 'year']]      F   \n",
       "\n",
       "                                   relevant_articles  \\\n",
       "0  {'18071699': 1, '16385408': 1, '20411164': 1, ...   \n",
       "1  {'18071012': 1, '17644941': 1, '25801243': 1, ...   \n",
       "2  {'21285059': 1, '27594777': 1, '26266346': 1, ...   \n",
       "3  {'25740387': 1, '20979469': 1, '19386089': 1, ...   \n",
       "4  {'17233833': 1, '26264237': 1, '27434055': 1, ...   \n",
       "\n",
       "                   similar_patients  \\\n",
       "0                                {}   \n",
       "1  {'5519313-1': 1, '8039716-1': 1}   \n",
       "2                                {}   \n",
       "3                                {}   \n",
       "4                                {}   \n",
       "\n",
       "                                Sampling_label_list  \\\n",
       "0                                         ['E03.9']   \n",
       "1                                         ['J44.9']   \n",
       "2  ['E03.9', 'J44.9', 'J84.10', 'I50.9', 'J45.909']   \n",
       "3                                ['E03.9', 'I50.9']   \n",
       "4                     ['J45.909', 'J44.9', 'E03.9']   \n",
       "\n",
       "                                       icd10_codes  \\\n",
       "0          {M85.8, R73.09, M25.551, E83.52, E21.0}   \n",
       "1                            {J44.9, J85.1, C88.8}   \n",
       "2            {J84.10, M33.1, J18.9, I73.00, M06.9}   \n",
       "3  {D64.9, R53.83, C78.00, Z51.11, C79.31, C34.92}   \n",
       "4                                          {J44.9}   \n",
       "\n",
       "                                    raw_llm_response  \\\n",
       "0  identified_codes=[ICD10Code(code='E21.0', code...   \n",
       "1  identified_codes=[ICD10Code(code='J44.9', code...   \n",
       "2  identified_codes=[ICD10Code(code='M33.1', code...   \n",
       "3  identified_codes=[ICD10Code(code='C34.92', cod...   \n",
       "4  identified_codes=[ICD10Code(code='J44.9', code...   \n",
       "\n",
       "                                            json_llm  \\\n",
       "0  [{'code': 'E21.0', 'code_type': 'Primary', 'de...   \n",
       "1  [{'code': 'J44.9', 'code_type': 'Primary', 'de...   \n",
       "2  [{'code': 'M33.1', 'code_type': 'Primary', 'de...   \n",
       "3  [{'code': 'C34.92', 'code_type': 'Primary', 'd...   \n",
       "4  [{'code': 'J44.9', 'code_type': 'Primary', 'de...   \n",
       "\n",
       "                                 labels target_codes_detected  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]                    {}   \n",
       "1  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]               {J44.9}   \n",
       "2  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]              {J84.10}   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]                    {}   \n",
       "4  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]               {J44.9}   \n",
       "\n",
       "  target_codes_detected_uptil_3  \\\n",
       "0                            {}   \n",
       "1                         {J44}   \n",
       "2                         {J84}   \n",
       "3                            {}   \n",
       "4                         {J44}   \n",
       "\n",
       "                               patient_notes_cleaned  \n",
       "0  27-year old male patient history progressive f...  \n",
       "1  63-year-old man present 4-year history progres...  \n",
       "2  33-year-old female present rheumatologist may ...  \n",
       "3  44-year-old female patient history smoking pre...  \n",
       "4  58-year-old white woman history emphysema chro...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat all the chunks in data_labelled/postive and data_labelled/ negative folder. \n",
    "import pandas as pd\n",
    "import os\n",
    "from nlp_utils import generate_labels_for_df\n",
    "from icd_codes import target_code_list\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import ast \n",
    "from nlp_utils import preprocess_clinical_text, validate_train_has_all_labels\n",
    "\n",
    "postive_files = os.listdir('data_labelled/positive')    \n",
    "negative_files = os.listdir('data_labelled/negative')\n",
    "all_postive_df = pd.concat([pd.read_csv(f'data_labelled/positive/{file}') for file in postive_files])\n",
    "all_negative_df = pd.concat([pd.read_csv(f'data_labelled/negative/{file}') for file in negative_files])\n",
    "\n",
    "all_data = pd.concat([all_postive_df, all_negative_df])\n",
    "all_data = generate_labels_for_df(all_data, target_code_list)\n",
    "all_data['icd10_codes'] = all_data['icd10_codes'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "all_data['patient_notes_cleaned'] = all_data['patient'].progress_apply(preprocess_clinical_text)\n",
    "model_train_data = all_data[['patient_notes_cleaned', 'target_codes_detected_uptil_3']]\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "all_data[:10].to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_col = 'patient'\n",
    "train_label_col = 'target_codes_detected_uptil_3'\n",
    "model_train_data = all_data[[train_col, train_label_col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   target_code  count\n",
      "0          I10    571\n",
      "1          E11    539\n",
      "2          E78    292\n",
      "3          I50    261\n",
      "4          I25    251\n",
      "5          E66    191\n",
      "6          I48    183\n",
      "7          J84    132\n",
      "8          J44    103\n",
      "9          E03    101\n",
      "10         E10     98\n",
      "11         J45     76\n"
     ]
    }
   ],
   "source": [
    "# all_data['target_codes_detected_uptil_3'] = all_data['target_codes_detected_uptil_3'].apply(lambda x: frozenset(x) if isinstance(x, set) else x)\n",
    "# print(all_data.groupby(['target_codes_detected_uptil_3'])['patient_id'].count())\n",
    "# print(all_data.shape)\n",
    "validate_train_has_all_labels(model_train_data, target_code_list, 'target_codes_detected_uptil_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1469, 2)\n",
      "(1931, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient</th>\n",
       "      <th>target_codes_detected_uptil_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Peter M; male, 36 years old, married. Father o...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A 7-year-old boy presented to our hospital com...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A 5-year-old female patient presented with a 2...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A 10-year-old male child, weighing 24 kg was p...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A 12-year-old female patient visited the hospi...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             patient  \\\n",
       "0  Peter M; male, 36 years old, married. Father o...   \n",
       "1  A 7-year-old boy presented to our hospital com...   \n",
       "2  A 5-year-old female patient presented with a 2...   \n",
       "3  A 10-year-old male child, weighing 24 kg was p...   \n",
       "4  A 12-year-old female patient visited the hospi...   \n",
       "\n",
       "  target_codes_detected_uptil_3  \n",
       "0                            {}  \n",
       "1                            {}  \n",
       "2                            {}  \n",
       "3                            {}  \n",
       "4                            {}  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 1608 no code detected \n",
    "# 800 there is code detected \n",
    "# we need tobalance the dataset again by randomly sampling 800 from no code detected and 800 from there is code detected\n",
    "# Separate the data into two groups based on the presence of detected codes\n",
    "no_code_detected = model_train_data[model_train_data['target_codes_detected_uptil_3'].apply(lambda x: len(x) == 0)]\n",
    "code_detected = model_train_data[model_train_data['target_codes_detected_uptil_3'].apply(lambda x: len(x) > 0)]\n",
    "print(code_detected.shape)\n",
    "print(no_code_detected.shape)\n",
    "\n",
    "# Randomly sample 800 from each group\n",
    "sampled_no_code = no_code_detected.sample(n=1200, random_state=42)\n",
    "sampled_code = code_detected.sample(n=1200, random_state=42)\n",
    "\n",
    "# Combine the sampled data\n",
    "balanced_all_data = pd.concat([sampled_no_code, sampled_code])\n",
    "# Update the all_data DataFrame with the balanced dataset\n",
    "new_data= balanced_all_data.reset_index(drop=True)\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (1920, 2)\n",
      "Validation data shape: (240, 2)\n",
      "Testing data shape: (240, 2)\n"
     ]
    }
   ],
   "source": [
    "# Update the train, validation, and test splits with the filtered data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_data = train_test_split(balanced_all_data, test_size=0.2, random_state=42)\n",
    "val_data, test_data = train_test_split( val_data, test_size=0.5, random_state=42)\n",
    "# Display the shapes of the datasets\n",
    "# validate_train_has_all_labels(train_data, target_code_list, 'target_codes_detected_uptil_3')\n",
    "print(\"Training data shape:\", train_data.shape)\n",
    "print(\"Validation data shape:\", val_data.shape)\n",
    "print(\"Testing data shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, hamming_loss, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Assuming you have your data loaded like this:\n",
    "# df = pd.read_csv('your_combined_data.csv')\n",
    "\n",
    "# Step 1: Prepare your features (patient text/notes)\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train = mlb.fit_transform(train_data[train_label_col])\n",
    "y_val = mlb.transform(val_data[train_label_col])\n",
    "y_test = mlb.transform(test_data[train_label_col])\n",
    "\n",
    "# Step 2: Define feature column\n",
    "X_train = train_data[train_col].values  \n",
    "X_val = val_data[train_col].values\n",
    "X_test = test_data[train_col].values\n",
    "\n",
    "param_grid = {\n",
    "    'clf__estimator__C': [0.01, 0.1, 1, 10],\n",
    "    'clf__estimator__penalty': ['l1', 'l2'],\n",
    "    'clf__estimator__solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "\n",
    "X_train = train_data[train_col].values  # Your clinical text data\n",
    "\n",
    "# To do Check multinomial binarizer later \n",
    "# Step 4: Create a pipeline with text vectorization and multilabel classification\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1, 2))),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression(class_weight='balanced', solver='liblinear', max_iter=2000)))\n",
    "])\n",
    "# Wrap Pipeline in GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='f1_micro', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Train GridSearchCV (finds best hyperparameters)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best Model after Grid Search\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Step 5: Evaluate on Validation Set\n",
    "y_val_pred = best_pipeline.predict(X_val)\n",
    "print(\"\\n🔹 Best Parameters:\", grid_search.best_params_)\n",
    "print(\"\\n🔹 Validation Hamming Loss:\", hamming_loss(y_val, y_val_pred))\n",
    "print(\"\\n🔹 Validation Accuracy:\", accuracy_score(y_val, y_val_pred))\n",
    "val_report = classification_report(y_val, y_val_pred, target_names=mlb.classes_, output_dict=True)\n",
    "val_report_df = pd.DataFrame(val_report).transpose()\n",
    "print(\"\\n🔹 Validation Classification Report:\\n\", val_report_df)\n",
    "\n",
    "\n",
    "# Step 6: Final Testing on Test Set\n",
    "y_test_pred = best_pipeline.predict(X_test)\n",
    "print(\"\\n🔹 Test Hamming Loss:\", hamming_loss(y_test, y_test_pred))\n",
    "print(\"\\n🔹 Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "report = classification_report(y_test, y_test_pred, target_names=mlb.classes_, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "print(\"\\n🔹 Test Classification Report:\\n\", report_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Test Classification Report:\n",
      "               precision    recall  f1-score  support\n",
      "E03            0.538462  0.875000  0.666667      8.0\n",
      "E10            0.263158  0.625000  0.370370      8.0\n",
      "E11            0.745763  0.916667  0.822430     48.0\n",
      "E66            0.642857  0.818182  0.720000     11.0\n",
      "E78            0.533333  0.727273  0.615385     22.0\n",
      "I10            0.759259  0.891304  0.820000     46.0\n",
      "I25            0.645161  0.869565  0.740741     23.0\n",
      "I48            0.818182  0.900000  0.857143     20.0\n",
      "I50            0.515152  0.850000  0.641509     20.0\n",
      "J44            0.538462  0.875000  0.666667      8.0\n",
      "J45            0.500000  0.857143  0.631579      7.0\n",
      "J84            0.533333  0.666667  0.592593     12.0\n",
      "micro avg      0.628571  0.849785  0.722628    233.0\n",
      "macro avg      0.586093  0.822650  0.678757    233.0\n",
      "weighted avg   0.650873  0.849785  0.733163    233.0\n",
      "samples avg    0.335069  0.407292  0.354279    233.0\n",
      "\n",
      "🔹 Test Hamming Loss: 0.05277777777777778\n",
      "\n",
      "🔹 Test Accuracy: 0.5875\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🔹 Test Classification Report:\\n\", report_df)\n",
    "print(\"\\n🔹 Test Hamming Loss:\", hamming_loss(y_test, y_test_pred))\n",
    "print(\"\\n🔹 Test Accuracy:\", accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_store/LR/logistic_regression_best_params.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the best model\n",
    "import joblib\n",
    "# Save the entire pipeline (including vectorizer & model)\n",
    "joblib.dump(best_pipeline, 'model_store/LR/logistic_regression_pipeline.pkl')\n",
    "joblib.dump(grid_search.best_estimator_, 'model_store/LR/logistic_regression_model.pkl')\n",
    "# Save the best parameters separately\n",
    "joblib.dump(grid_search.best_params_, 'model_store/LR/logistic_regression_best_params.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top features for label I10:\n",
      "                Coefficient\n",
      "hypothyroidism   125.099262\n",
      "thyroid           26.702011\n",
      "levothyroxine     14.198811\n",
      "dermatitis        11.349854\n",
      "disease            8.416013\n",
      "µmol               7.003786\n",
      "normal             6.079244\n",
      "iaad               5.982817\n",
      "of                 5.638004\n",
      "anti               5.006909\n",
      "\n",
      "Top features for label I25.10:\n",
      "               Coefficient\n",
      "type diabetes    46.261391\n",
      "insulin          31.495627\n",
      "dependent        24.086648\n",
      "t1dm             17.078835\n",
      "transplanted     11.858344\n",
      "diabetic         10.051524\n",
      "eyes              9.003539\n",
      "lvef              8.451259\n",
      "and weight        8.340949\n",
      "glucose           8.002659\n",
      "\n",
      "Top features for label I50.9:\n",
      "                   Coefficient\n",
      "diabetes            107.421378\n",
      "diabetes mellitus    27.821489\n",
      "diabetic             18.948758\n",
      "pd                    3.374331\n",
      "mg                    2.988256\n",
      "to                    2.772602\n",
      "reference             1.448283\n",
      "and                   1.426257\n",
      "with                  1.101892\n",
      "wound                 1.040318\n",
      "\n",
      "Top features for label I48.91:\n",
      "             Coefficient\n",
      "obesity       118.635627\n",
      "obese          34.196733\n",
      "bmi            16.177138\n",
      "weight gain    13.045111\n",
      "venous          8.831862\n",
      "compliance      7.088623\n",
      "pancreatic      7.046939\n",
      "reported        6.670064\n",
      "years old       5.969686\n",
      "uc              5.773816\n",
      "\n",
      "Top features for label J44.9:\n",
      "                Coefficient\n",
      "hyperlipidemia    72.451732\n",
      "dyslipidemia      61.961266\n",
      "hypertension      29.814359\n",
      "lad                8.810372\n",
      "type               8.369603\n",
      "family             8.227139\n",
      "metformin          7.736246\n",
      "cholesterol        5.398088\n",
      "mmol               4.623818\n",
      "liver              4.405346\n",
      "\n",
      "Top features for label J45.909:\n",
      "                 Coefficient\n",
      "hypertension      120.648608\n",
      "hypertensive        8.330013\n",
      "reference range     4.072056\n",
      "the patient         3.421287\n",
      "coronary            3.057461\n",
      "to                  2.790664\n",
      "and                 2.634553\n",
      "mg                  2.385463\n",
      "per minute          1.835641\n",
      "she was             1.664020\n",
      "\n",
      "Top features for label J84.10:\n",
      "                 Coefficient\n",
      "heart disease      41.498659\n",
      "artery disease     27.931089\n",
      "coronary           22.590577\n",
      "myocardial         22.095457\n",
      "coronary artery    13.117831\n",
      "aspirin            11.319366\n",
      "lad                 6.510446\n",
      "cardiac             6.033947\n",
      "descending          5.666381\n",
      "disease             5.461015\n",
      "\n",
      "Top features for label E10.9:\n",
      "                     Coefficient\n",
      "atrial fibrillation    81.919291\n",
      "arrhythmia             29.586252\n",
      "atrial                 22.037476\n",
      "af                     19.589780\n",
      "fibrillation           13.756050\n",
      "warfarin               11.791792\n",
      "echocardiogram tte      8.494617\n",
      "per minute              7.805869\n",
      "la                      7.238847\n",
      "sao2                    6.835903\n",
      "\n",
      "Top features for label E11.9:\n",
      "               Coefficient\n",
      "heart failure    40.003579\n",
      "heart            19.279568\n",
      "fraction         18.655596\n",
      "failure          17.532885\n",
      "ventricular       9.682150\n",
      "cardiac           9.444667\n",
      "systolic          6.554138\n",
      "shortness of      5.933200\n",
      "aortic            5.498565\n",
      "dyspnea           5.215861\n",
      "\n",
      "Top features for label E03.9:\n",
      "                       Coefficient\n",
      "chronic obstructive      47.896514\n",
      "copd                     30.367298\n",
      "obstructive              29.862065\n",
      "chronic                  17.258609\n",
      "respiratory              14.569850\n",
      "heart failure            13.474538\n",
      "came                     12.758672\n",
      "obstructive pulmonary    12.575334\n",
      "lung                     12.401082\n",
      "poisoning                10.554134\n",
      "\n",
      "Top features for label E78.5:\n",
      "           Coefficient\n",
      "asthma      133.115549\n",
      "airway       13.677219\n",
      "fev1         12.426968\n",
      "lip           9.424990\n",
      "ecmo          9.193805\n",
      "injection     8.729030\n",
      "ed            7.734617\n",
      "iaad          6.142197\n",
      "and           3.925228\n",
      "vaginal       2.976632\n",
      "\n",
      "Top features for label E66.9:\n",
      "                    Coefficient\n",
      "pulmonary fibrosis    79.968756\n",
      "interstitial          38.807426\n",
      "lavage                12.651642\n",
      "lung                  12.400406\n",
      "mg orally             11.520361\n",
      "respiratory            9.933938\n",
      "capacity               9.229934\n",
      "pulmonary              7.800039\n",
      "and                    7.752585\n",
      "oxygen                 6.145370\n"
     ]
    }
   ],
   "source": [
    "# Access the logistic regression model\n",
    "logistic_model = pipeline.named_steps['clf'].estimator  # Get the underlying LogisticRegression model\n",
    "\n",
    "# Get the feature names from the TF-IDF vectorizer\n",
    "tfidf_vectorizer = best_pipeline.named_steps['tfidf']\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Access the OneVsRestClassifier model from the best pipeline\n",
    "ovr_model = best_pipeline.named_steps['clf']\n",
    "\n",
    "# Get the coefficients for each class\n",
    "coefficients = ovr_model.estimators_  # This is a list of LogisticRegression models\n",
    "\n",
    "# Create a DataFrame to display feature names and their corresponding coefficients\n",
    "for i, label in enumerate(target_code_list):  # Replace with your actual labels\n",
    "    print(f\"\\nTop features for label {label}:\")\n",
    "    class_coef_df = pd.DataFrame(coefficients[i].coef_, columns=feature_names).transpose()  # Get coefficients for the i-th class\n",
    "    class_coef_df.columns = ['Coefficient']\n",
    "    class_coef_df = class_coef_df.sort_values(by='Coefficient', ascending=False)  # Sort by coefficient value\n",
    "    print(class_coef_df.head(10))  # Display top 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Function to Predict ICD-10 Codes on New Text Data\n",
    "def predict_icd_codes(text, pipeline=best_pipeline, mlb=mlb):\n",
    "    \"\"\"Predict ICD-10 codes for a given clinical text.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        text = [text]  # Convert single text input to list\n",
    "    \n",
    "    # Predict using trained pipeline\n",
    "    binary_preds = pipeline.predict(text)\n",
    "    \n",
    "    # Convert binary predictions back to ICD-10 codes\n",
    "    predicted_codes = mlb.inverse_transform(binary_preds)\n",
    "    \n",
    "    return predicted_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_utils import preprocess_clinical_text\n",
    "example_text = \"A 2-year-old girl had diffusely distributed rashes followed by abdominal pain, non-bloody loose stools (once a day, which resolved in 2 days), anasarca, and anuria for a month. The patient was admitted to a local hospital on October 18, 2018 with anemia (hemoglobin, 87 g/L), thrombocytopenia (platelet, PLT 82 × 109/L), stage 3 acute kidney injury (AKI) according to the guidelines of Kidney Disease: Improving Global Outcomes (KDIGO) (serum creatinine 961 μmol/L; creatinine clearance, 4.6 ml/min/1.73m2; and blood urea nitrogen 77.2 mmol/L) (). Initial urinalysis showed urine protein 3+; 24-h proteinuria was 0.16 g (24-h urine amount: 40 ml), and other blood biochemistry analyses demonstrated hypoalbuminemia (albumin, ALB 30.5 g/L) and hyperlipidemia (total cholesterol, TC 6.42 mmol/L). There was no abdominal pain, loose stools, or vomiting. Continuous renal replacement therapy (CRRT) was initiated the first time. The child was transferred to our hospital 7 days later. Physical examination revealed hypertension (blood pressure was initially normal), pallor, generalized edema, and hepatomegaly. Routine blood tests showed moderate anemia with increased reticulocyte percentage (5.39%), while PLT returned to baseline levels. Urinalysis showed 1+ hematuria and 3+ proteinuria but improved hypoalbuminemia (ALB 31 g/L) and hyperlipidemia (TC, 5.8 mmol/L). Other blood chemistry analyses were consistent with KDIGO stage 3 AKI, and lactate dehydrogenase was elevated (386 U/L). Helmet erythrocytes were observed in peripheral blood smears. Coombs test and paroxysmal nocturnal hemoglobinuria screening tests were negative. Platelet activation tests and complement C3 levels were normal. Stool culture was negative for Escherichia coli. aHUS was, thus, diagnosed. Therefore, the patient underwent alternate-day CRRT and six sequences of therapeutic plasma exchange. Hemolysis still progressed, and renal function did not recover after treatment (). Eculizumab was inaccessible since it had just been approved in the Chinese market, and renal biopsy was not performed due to coagulation issues. However, further measures were taken to determine the cause of aHUS. CFH autoantibodies were negative. Next-generation sequencing assay for the nephrology panel (MyGenostics) was performed using peripheral blood samples from the patient and her parents. Genes that are known to be associated with aHUS, including complement regulatory protein-coding genes and other related genes (C3, C4, C5, CFB, CFH, CFI CFHR1, CFHR3, CFHR4, CFHR5, THBD, PLG, and DGKE), were included. However, no variants were detected. Unexpectedly, a de novo heterozygous c.754G>A missense variant in exon 9 of the WT1 gene (NM_001198551.1: c.754G>A) was detected, resulting in a p. Asp252Asn substitution (NP_001185480.1: p. Asp252Asn), and this was further verified by Sanger sequencing (). Variant analysis according to the American College of Medical Genetics and Genomics standards and guidelines revealed PS1 (sequence variation had previously been reported to be pathogenic for DDS in the Human Gene Mutation Database) + PS2 (de novo and no family history) + PM2 (sequence variation absent from controls), which was regarded as pathogenic for DDS (RefSNP number: rs28941778). The karyotype of this patient was suspected to be 46, XY. As described earlier, DDS is a condition characterized by the triad of nephropathy, genitalia abnormality, and a high risk of Wilm's tumor (). The patient was a female but later proved to be a genetic male owing to the 46, XY karyotype; however, normally developed female external genitalia was observed. Abdominal computed tomography did not reveal cryptorchidism or abnormal renal mass. There was no improvement in renal function following therapy; hence, the patient received regular peritoneal dialysis and underwent renal transplantation 1 year later; however, prior to that, bilateral nephrectomy was performed for the prevention of Wilm's tumor. Gross specimen examination demonstrated many dysplastic glomeruli and partial sclerotic glomeruli, protein casts, and calcium depositions in the renal tubes, with vacuolar degeneration in the epithelium of the proximal tubes, thickened arteriolar walls and narrowed lumens, and interstitial inflammation infiltration. There was no recurrence of aHUS 10 months after transplantation.\"\n",
    "res = preprocess_clinical_text(example_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "sample_text = \"Patient has hypertension and obesity.\"\n",
    "predicted_codes = predict_icd_codes(sample_text)\n",
    "print(f\"\\nPredicted codes for '{sample_text}':\", predicted_codes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Based Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length: 756.1661764705882\n",
      "Max length: 3906\n",
      "Min length: 32\n",
      "% exceeding 512 tokens: 68.15%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Check token lengths\n",
    "\n",
    "token_lengths = []\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "for text in model_train_data[train_col].values:\n",
    "    tokens = tokenizer.encode(text)\n",
    "    token_lengths.append(len(tokens))\n",
    "\n",
    "# Basic statistics\n",
    "import numpy as np\n",
    "print(f\"Average length: {np.mean(token_lengths)}\")\n",
    "print(f\"Max length: {np.max(token_lengths)}\")\n",
    "print(f\"Min length: {np.min(token_lengths)}\")\n",
    "print(f\"% exceeding 512 tokens: {np.mean(np.array(token_lengths) > 512) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, hamming_loss\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "class ChunkedTextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128, stride=32):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.stride = stride\n",
    "        \n",
    "        # Process all texts and create a mapping from chunk index to original text index\n",
    "        self.chunks = []\n",
    "        self.chunk_to_text_idx = []\n",
    "        \n",
    "        for idx, text in enumerate(texts):\n",
    "            # Tokenize the text\n",
    "            tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "            \n",
    "            # If text is shorter than max_length, just keep it as is\n",
    "            if len(tokens) <= max_length - 2:  # -2 for [CLS] and [SEP]\n",
    "                self.chunks.append(text)\n",
    "                self.chunk_to_text_idx.append(idx)\n",
    "            else:\n",
    "                # Split into overlapping chunks\n",
    "                # -2 to account for [CLS] and [SEP] tokens\n",
    "                chunk_size = max_length - 2\n",
    "                \n",
    "                # Calculate number of chunks\n",
    "                num_chunks = (len(tokens) - chunk_size) // stride + 2\n",
    "                \n",
    "                # Get chunks with stride\n",
    "                for i in range(num_chunks):\n",
    "                    start = i * stride\n",
    "                    end = min(start + chunk_size, len(tokens))\n",
    "                    \n",
    "                    # If we've reached the end, break\n",
    "                    if start >= len(tokens):\n",
    "                        break\n",
    "                    \n",
    "                    # Extract chunk tokens and convert back to text\n",
    "                    chunk_tokens = tokens[start:end]\n",
    "                    chunk_text = tokenizer.decode(chunk_tokens)\n",
    "                    \n",
    "                    self.chunks.append(chunk_text)\n",
    "                    self.chunk_to_text_idx.append(idx)\n",
    "        \n",
    "        # Store original labels for later use\n",
    "        self.text_labels = labels\n",
    "        \n",
    "        # Create labels for chunks (same label for all chunks from same text)\n",
    "        self.chunk_labels = [labels[self.chunk_to_text_idx[i]] for i in range(len(self.chunks))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.chunks[idx]\n",
    "        label = self.chunk_labels[idx]\n",
    "        \n",
    "        # Tokenize with special tokens\n",
    "        encoding = self.tokenizer(\n",
    "            chunk,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Remove the batch dimension\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float),\n",
    "            'text_idx': self.chunk_to_text_idx[idx]\n",
    "        }\n",
    "        \n",
    "        return item\n",
    "    \n",
    "    def get_original_indices(self):\n",
    "        \"\"\"Returns a list mapping from original text indices to chunk indices\"\"\"\n",
    "        text_to_chunks = {}\n",
    "        for chunk_idx, text_idx in enumerate(self.chunk_to_text_idx):\n",
    "            if text_idx not in text_to_chunks:\n",
    "                text_to_chunks[text_idx] = []\n",
    "            text_to_chunks[text_idx].append(chunk_idx)\n",
    "        \n",
    "        return text_to_chunks\n",
    "\n",
    "\n",
    "class BERTClassifierHead:\n",
    "    def __init__(self, model_name='emilyalsentzer/Bio_ClinicalBERT', num_labels=2, \n",
    "                 max_length=128, stride=32, batch_size=32, num_epochs=3, lr=2e-5,\n",
    "                 aggregation='max'):\n",
    "        self.model_name = model_name\n",
    "        self.num_labels = num_labels\n",
    "        self.max_length = max_length\n",
    "        self.stride = stride\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.lr = lr\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.aggregation = aggregation  # 'max', 'mean', or 'weighted'\n",
    "        \n",
    "        # Initialize tokenizer and model\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.class_names = None\n",
    "        \n",
    "    def prepare_data(self, X_train, y_train, X_val=None, y_val=None, X_test=None, y_test=None, class_names=None):\n",
    "        \"\"\"Prepare datasets and dataloaders with chunking for long documents\"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.class_names = class_names\n",
    "        \n",
    "        # Create datasets with chunking\n",
    "        train_dataset = ChunkedTextDataset(X_train, y_train, self.tokenizer, \n",
    "                                         self.max_length, self.stride)\n",
    "        self.train_dataloader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.train_text_to_chunks = train_dataset.get_original_indices()\n",
    "        \n",
    "        if X_val is not None and y_val is not None:\n",
    "            val_dataset = ChunkedTextDataset(X_val, y_val, self.tokenizer, \n",
    "                                           self.max_length, self.stride)\n",
    "            self.val_dataloader = DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "            self.val_text_to_chunks = val_dataset.get_original_indices()\n",
    "        else:\n",
    "            self.val_dataloader = None\n",
    "            self.val_text_to_chunks = None\n",
    "            \n",
    "        if X_test is not None and y_test is not None:\n",
    "            test_dataset = ChunkedTextDataset(X_test, y_test, self.tokenizer, \n",
    "                                            self.max_length, self.stride)\n",
    "            self.test_dataloader = DataLoader(test_dataset, batch_size=self.batch_size)\n",
    "            self.test_text_to_chunks = test_dataset.get_original_indices()\n",
    "        else:\n",
    "            self.test_dataloader = None\n",
    "            self.test_text_to_chunks = None\n",
    "    \n",
    "    def freeze_bert_base(self):\n",
    "        \"\"\"Freeze all parameters of the base BERT model except the classification head\"\"\"\n",
    "        for param in self.model.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Make sure the classification head is trainable\n",
    "        for param in self.model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        # Summary of trainable parameters\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print(f\"Total parameters: {total_params}\")\n",
    "        print(f\"Trainable parameters: {trainable_params} ({trainable_params/total_params:.2%})\")\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None, class_names=None):\n",
    "        \"\"\"Fine-tune only the classification head of the BERT model\"\"\"\n",
    "        # Prepare data with chunking\n",
    "        self.prepare_data(X_train, y_train, X_val, y_val, class_names=class_names)\n",
    "        \n",
    "        # Initialize model for sequence classification\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name, \n",
    "            num_labels=self.num_labels,\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # Freeze the base BERT model, only train the classification head\n",
    "        self.freeze_bert_base()\n",
    "        \n",
    "        # Only optimize parameters that require gradients\n",
    "        optimizer = AdamW(\n",
    "            [p for p in self.model.parameters() if p.requires_grad],\n",
    "            lr=self.lr\n",
    "        )\n",
    "        \n",
    "        total_steps = len(self.train_dataloader) * self.num_epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, \n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        # Define loss function for multi-label classification\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        # Training loop\n",
    "        best_val_f1 = 0\n",
    "        for epoch in range(self.num_epochs):\n",
    "            print(f\"Epoch {epoch+1}/{self.num_epochs}\")\n",
    "            \n",
    "            # Training\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            train_steps = 0\n",
    "            \n",
    "            for batch in tqdm(self.train_dataloader, desc=\"Training\"):\n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                \n",
    "                logits = outputs.logits\n",
    "                loss = loss_fn(logits, labels)\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                train_steps += 1\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "            \n",
    "            avg_train_loss = train_loss / train_steps\n",
    "            print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "            \n",
    "            # Validation\n",
    "            if self.val_dataloader:\n",
    "                val_metrics = self.evaluate(self.val_dataloader, self.val_text_to_chunks)\n",
    "                print(f\"Validation metrics: {val_metrics}\")\n",
    "                \n",
    "                # Save best model based on F1 score\n",
    "                if val_metrics['f1_macro'] > best_val_f1:\n",
    "                    best_val_f1 = val_metrics['f1_macro']\n",
    "                    self.save_model(\"best_model\")\n",
    "                    print(\"Saved best model!\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _aggregate_chunk_predictions(self, chunk_logits, chunk_to_text, num_texts):\n",
    "        \"\"\"Aggregate predictions from multiple chunks of the same text\"\"\"\n",
    "        # Initialize arrays for aggregated predictions\n",
    "        # Shape: [num_texts, num_labels]\n",
    "        if self.aggregation == 'max':\n",
    "            aggregated_logits = np.full((num_texts, self.num_labels), -np.inf)\n",
    "        else:  # mean or weighted\n",
    "            aggregated_logits = np.zeros((num_texts, self.num_labels))\n",
    "            counts = np.zeros(num_texts)\n",
    "        \n",
    "        # Aggregate logits from chunks\n",
    "        for text_idx, chunk_indices in chunk_to_text.items():\n",
    "            if self.aggregation == 'max':\n",
    "                # For each label, take the max logit across all chunks of this text\n",
    "                for chunk_idx in chunk_indices:\n",
    "                    chunk_logit = chunk_logits[chunk_idx]\n",
    "                    aggregated_logits[text_idx] = np.maximum(aggregated_logits[text_idx], chunk_logit)\n",
    "            elif self.aggregation == 'mean':\n",
    "                # For each label, take the mean logit across all chunks of this text\n",
    "                for chunk_idx in chunk_indices:\n",
    "                    aggregated_logits[text_idx] += chunk_logits[chunk_idx]\n",
    "                    counts[text_idx] += 1\n",
    "                \n",
    "                # Avoid division by zero\n",
    "                if counts[text_idx] > 0:\n",
    "                    aggregated_logits[text_idx] /= counts[text_idx]\n",
    "            elif self.aggregation == 'weighted':\n",
    "                # Give more weight to chunks with higher confidence\n",
    "                for chunk_idx in chunk_indices:\n",
    "                    chunk_logit = chunk_logits[chunk_idx]\n",
    "                    # Calculate weight based on the max absolute logit value\n",
    "                    weight = np.max(np.abs(chunk_logit))\n",
    "                    aggregated_logits[text_idx] += chunk_logit * weight\n",
    "                    counts[text_idx] += weight\n",
    "                \n",
    "                # Avoid division by zero\n",
    "                if counts[text_idx] > 0:\n",
    "                    aggregated_logits[text_idx] /= counts[text_idx]\n",
    "        \n",
    "        # Apply sigmoid to get probabilities\n",
    "        aggregated_probs = 1 / (1 + np.exp(-aggregated_logits))\n",
    "        \n",
    "        # Convert to binary predictions\n",
    "        aggregated_preds = (aggregated_probs >= 0.3).astype(int)\n",
    "        \n",
    "        return aggregated_preds, aggregated_probs\n",
    "    \n",
    "    def predict(self, X, return_probabilities=False):\n",
    "        \"\"\"Predict using the fine-tuned model with sliding window approach\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Prepare data with chunking\n",
    "        dataset = ChunkedTextDataset(\n",
    "            X, \n",
    "            np.zeros((len(X), self.num_labels)), \n",
    "            self.tokenizer, \n",
    "            self.max_length, \n",
    "            self.stride\n",
    "        )\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size)\n",
    "        text_to_chunks = dataset.get_original_indices()\n",
    "        \n",
    "        all_chunk_logits = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                \n",
    "                logits = outputs.logits.cpu().numpy()\n",
    "                all_chunk_logits.append(logits)\n",
    "        \n",
    "        all_chunk_logits = np.vstack(all_chunk_logits)\n",
    "        \n",
    "        # Aggregate predictions from chunks\n",
    "        predictions, probabilities = self._aggregate_chunk_predictions(\n",
    "            all_chunk_logits, text_to_chunks, len(X)\n",
    "        )\n",
    "        \n",
    "        if return_probabilities:\n",
    "            return predictions, probabilities\n",
    "        else:\n",
    "            return predictions\n",
    "    \n",
    "    def evaluate(self, dataloader, text_to_chunks):\n",
    "        \"\"\"Evaluate the model on the given dataloader with chunk aggregation\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Get unique text indices to count original documents\n",
    "        text_indices = set()\n",
    "        for indices in text_to_chunks.values():\n",
    "            for idx in indices:\n",
    "                text_indices.add(idx)\n",
    "        num_texts = max(text_indices) + 1\n",
    "        \n",
    "        all_chunk_logits = []\n",
    "        all_chunk_labels = []\n",
    "        all_text_indices = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].cpu().numpy()\n",
    "                text_idx = batch['text_idx'].cpu().numpy()\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                \n",
    "                logits = outputs.logits.cpu().numpy()\n",
    "                \n",
    "                all_chunk_logits.append(logits)\n",
    "                all_chunk_labels.append(labels)\n",
    "                all_text_indices.append(text_idx)\n",
    "        \n",
    "        all_chunk_logits = np.vstack(all_chunk_logits)\n",
    "        all_chunk_labels = np.vstack(all_chunk_labels)\n",
    "        all_text_indices = np.concatenate(all_text_indices)\n",
    "        \n",
    "        # Aggregate predictions from chunks\n",
    "        aggregated_preds, _ = self._aggregate_chunk_predictions(\n",
    "            all_chunk_logits, text_to_chunks, num_texts\n",
    "        )\n",
    "        \n",
    "        # Get the original labels (taking the first chunk's label for each text)\n",
    "        original_labels = np.zeros((num_texts, self.num_labels))\n",
    "        for text_idx, chunk_indices in text_to_chunks.items():\n",
    "            if chunk_indices:  # If there's at least one chunk\n",
    "                chunk_idx = chunk_indices[0]  # Take the first chunk's index\n",
    "                original_labels[text_idx] = all_chunk_labels[chunk_idx]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            original_labels, aggregated_preds, average='macro', zero_division=0\n",
    "        )\n",
    "        precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(\n",
    "            original_labels, aggregated_preds, average='micro', zero_division=0\n",
    "        )\n",
    "        \n",
    "        # For multi-label, we use hamming score (1 - hamming loss) as accuracy equivalent\n",
    "        h_loss = hamming_loss(original_labels, aggregated_preds)\n",
    "        hamming_score = 1 - h_loss\n",
    "        \n",
    "        # Calculate per-class metrics\n",
    "        class_precision, class_recall, class_f1, _ = precision_recall_fscore_support(\n",
    "            original_labels, aggregated_preds, average=None, zero_division=0\n",
    "        )\n",
    "        \n",
    "        metrics = {\n",
    "            'hamming_score': hamming_score,\n",
    "            'hamming_loss': h_loss,\n",
    "            'precision_macro': precision,\n",
    "            'recall_macro': recall,\n",
    "            'f1_macro': f1,\n",
    "            'precision_micro': precision_micro,\n",
    "            'recall_micro': recall_micro,\n",
    "            'f1_micro': f1_micro,\n",
    "            'class_precision': class_precision,\n",
    "            'class_recall': class_recall,\n",
    "            'class_f1': class_f1\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        \"\"\"Save the model, tokenizer, and configuration\"\"\"\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        \n",
    "        self.model.save_pretrained(path)\n",
    "        self.tokenizer.save_pretrained(path)\n",
    "        \n",
    "        # Save additional configuration\n",
    "        config = {\n",
    "            'model_name': self.model_name,\n",
    "            'num_labels': self.num_labels,\n",
    "            'max_length': self.max_length,\n",
    "            'stride': self.stride,\n",
    "            'aggregation': self.aggregation,\n",
    "            'class_names': self.class_names\n",
    "        }\n",
    "\n",
    "                # Convert any NumPy arrays in the config to lists\n",
    "        def convert_to_serializable(obj):\n",
    "            if isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()  # Convert NumPy array to list\n",
    "            elif isinstance(obj, list):\n",
    "                return [convert_to_serializable(item) for item in obj]  # Recursively convert lists\n",
    "            elif isinstance(obj, dict):\n",
    "                return {key: convert_to_serializable(value) for key, value in obj.items()}  # Recursively convert dicts\n",
    "            return obj  # Return as is if it's already serializable\n",
    "\n",
    "        # Convert the entire config dictionary\n",
    "        config = convert_to_serializable(config)\n",
    "\n",
    "        \n",
    "        with open(os.path.join(path, 'config.json'), 'w') as f:\n",
    "            import json\n",
    "            json.dump(config, f)\n",
    "            \n",
    "    @classmethod\n",
    "    def load_model(cls, path):\n",
    "        \"\"\"Load a saved model\"\"\"\n",
    "        # Load configuration\n",
    "        with open(os.path.join(path, 'config.json'), 'r') as f:\n",
    "            import json\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Create a new instance\n",
    "        classifier = cls(\n",
    "            model_name=path,  # Use the path as the model name to load from local\n",
    "            num_labels=config['num_labels'],\n",
    "            max_length=config['max_length'],\n",
    "            stride=config.get('stride', 256),\n",
    "            aggregation=config.get('aggregation', 'max')\n",
    "        )\n",
    "        \n",
    "        # Load tokenizer and model\n",
    "        classifier.tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "        classifier.model = AutoModelForSequenceClassification.from_pretrained(path)\n",
    "        classifier.class_names = config.get('class_names', None)\n",
    "        classifier.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        classifier.model.to(classifier.device)\n",
    "        \n",
    "        return classifier\n",
    "\n",
    "\n",
    "# Example usage with full training and evaluation pipeline\n",
    "def train_and_evaluate_bert_classifier(X_train, y_train, X_val, y_val, X_test, y_test, class_names=None):\n",
    "    # Get number of labels from y_train\n",
    "    num_labels = y_train.shape[1]\n",
    "    \n",
    "    # Initialize the classifier\n",
    "    classifier = BERTClassifierHead(\n",
    "        model_name='emilyalsentzer/Bio_ClinicalBERT',\n",
    "        num_labels=num_labels,\n",
    "        max_length=128,  # Maximum BERT context size\n",
    "        stride=32,      # 50% overlap between chunks\n",
    "        batch_size=32,\n",
    "        num_epochs=3,\n",
    "        lr=2e-5,\n",
    "        aggregation='max'  # 'max', 'mean', or 'weighted'\n",
    "    )\n",
    "    \n",
    "    # Train the model (will only train the classification head)\n",
    "    classifier.fit(X_train, y_train, X_val, y_val, class_names=class_names)\n",
    "    \n",
    "    # Prepare test data if not already done\n",
    "    if classifier.test_dataloader is None:\n",
    "        classifier.prepare_data(X_train, y_train, X_val, y_val, X_test, y_test, class_names=class_names)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_metrics = classifier.evaluate(classifier.test_dataloader, classifier.test_text_to_chunks)\n",
    "    \n",
    "    \n",
    "    print(f\"Hamming Loss: {test_metrics['hamming_loss']:.4f}\")\n",
    "    print(f\"Hamming Score: {test_metrics['hamming_score']:.4f}\")\n",
    "    print(f\"Macro F1: {test_metrics['f1_macro']:.4f}\")\n",
    "    print(f\"Micro F1: {test_metrics['f1_micro']:.4f}\")\n",
    "\n",
    "    results = {\n",
    "        'Hamming Loss': test_metrics['hamming_loss'],\n",
    "        'Hamming Score': test_metrics['hamming_score'],\n",
    "        'Macro F1': test_metrics['f1_macro'],\n",
    "        'Micro F1': test_metrics['f1_micro'],\n",
    "        'Class Metrics': class_metrics\n",
    "    }\n",
    "\n",
    "    print(\"\\n🔹 Test Metrics:\")\n",
    "    print(results)\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    \n",
    "    # Format detailed per-class metrics as a DataFrame\n",
    "    class_metrics = {\n",
    "        'Precision': test_metrics['class_precision'],\n",
    "        'Recall': test_metrics['class_recall'],\n",
    "        'F1': test_metrics['class_f1']\n",
    "    }\n",
    "    \n",
    "    class_df = pd.DataFrame(class_metrics)\n",
    "    if class_names is not None:\n",
    "        class_df.index = class_names\n",
    "    else:\n",
    "        class_df.index = [f'Label {i}' for i in range(num_labels)]\n",
    "    \n",
    "    print(\"\\n🔹 Per-class Metrics:\")\n",
    "    print(class_df)\n",
    "    \n",
    "    # Get classification report for test set\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    print(\"\\n🔹 Classification Report:\")\n",
    "    if class_names is not None:\n",
    "        print(classification_report(y_test, y_pred, target_names=class_names, zero_division=0))\n",
    "    else:\n",
    "        print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    \n",
    "    # Save the final model\n",
    "    classifier.save_model(\"model_store/Clinical_BERT/final_model\")\n",
    "    # Save metrics to CSV\n",
    "    results_df.to_csv(\"model_store/Clinical_BERT/evaluation_results.csv\", index=False)\n",
    "\n",
    "    # Optionally, save class metrics separately\n",
    "    class_df.to_csv(\"model_store/Clinical_BERT/class_metrics.csv\")\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "\n",
    "# Example of how to use the saved model for inference\n",
    "def use_saved_model(model_path, new_texts):\n",
    "    # Load the saved model\n",
    "    classifier = BERTClassifierHead.load_model(model_path)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions, probabilities = classifier.predict(new_texts, return_probabilities=True)\n",
    "    \n",
    "    # Format results\n",
    "    results = []\n",
    "    for i, text in enumerate(new_texts):\n",
    "        result = {\n",
    "            'text': text[:100] + '...' if len(text) > 100 else text,\n",
    "            'predictions': {}\n",
    "        }\n",
    "        \n",
    "        for j, label in enumerate(classifier.class_names or [f'Label {j}' for j in range(classifier.num_labels)]):\n",
    "            result['predictions'][label] = {\n",
    "                'predicted': bool(predictions[i, j]),\n",
    "                'probability': float(probabilities[i, j])\n",
    "            }\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 108319500\n",
      "Trainable parameters: 9228 (0.01%)\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 65/65 [02:29<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.5714\n",
      "Validation metrics: {'hamming_score': 0.9555609915198956, 'hamming_loss': 0.04443900848010437, 'precision_macro': 0.04687074829931973, 'recall_macro': 0.75, 'f1_macro': 0.08509790591281967, 'precision_micro': 0.04886561954624782, 'recall_micro': 1.0, 'f1_micro': 0.09317803660565724, 'class_precision': array([0.        , 0.02      , 0.12244898, 0.06      , 0.06      ,\n",
      "       0.18      , 0.04      , 0.04      , 0.02      , 0.02      ,\n",
      "       0.        , 0.        ]), 'class_recall': array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.]), 'class_f1': array([0.        , 0.03921569, 0.21818182, 0.11320755, 0.11320755,\n",
      "       0.30508475, 0.07692308, 0.07692308, 0.03921569, 0.03921569,\n",
      "       0.        , 0.        ])}\n",
      "Saved best model!\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 65/65 [02:46<00:00,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.4910\n",
      "Validation metrics: {'hamming_score': 0.9594748858447488, 'hamming_loss': 0.04052511415525114, 'precision_macro': 0.05032931354359926, 'recall_macro': 0.7222222222222222, 'f1_macro': 0.08903861627381572, 'precision_micro': 0.04990403071017274, 'recall_micro': 0.9285714285714286, 'f1_micro': 0.0947176684881603, 'class_precision': array([0.        , 0.02      , 0.16      , 0.06      , 0.06122449,\n",
      "       0.18      , 0.04      , 0.04      , 0.02272727, 0.02      ,\n",
      "       0.        , 0.        ]), 'class_recall': array([0.        , 1.        , 0.66666667, 1.        , 1.        ,\n",
      "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
      "       0.        , 0.        ]), 'class_f1': array([0.        , 0.03921569, 0.25806452, 0.11320755, 0.11538462,\n",
      "       0.30508475, 0.07692308, 0.07692308, 0.04444444, 0.03921569,\n",
      "       0.        , 0.        ])}\n",
      "Saved best model!\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 65/65 [02:54<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.4606\n",
      "Validation metrics: {'hamming_score': 0.9612687540769732, 'hamming_loss': 0.03873124592302674, 'precision_macro': 0.05453373015873016, 'recall_macro': 0.7083333333333334, 'f1_macro': 0.09296057261090733, 'precision_micro': 0.05030181086519115, 'recall_micro': 0.8928571428571429, 'f1_micro': 0.09523809523809523, 'class_precision': array([0.        , 0.02083333, 0.2       , 0.0625    , 0.0625    ,\n",
      "       0.18      , 0.04      , 0.04      , 0.02857143, 0.02      ,\n",
      "       0.        , 0.        ]), 'class_recall': array([0. , 1. , 0.5, 1. , 1. , 1. , 1. , 1. , 1. , 1. , 0. , 0. ]), 'class_f1': array([0.        , 0.04081633, 0.28571429, 0.11764706, 0.11764706,\n",
      "       0.30508475, 0.07692308, 0.07692308, 0.05555556, 0.03921569,\n",
      "       0.        , 0.        ])}\n",
      "Saved best model!\n",
      "Hamming Loss: 0.0379\n",
      "Hamming Score: 0.9621\n",
      "Macro F1: 0.1317\n",
      "Micro F1: 0.1421\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'class_metrics' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m classifier = \u001b[43mtrain_and_evaluate_bert_classifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use only the first 100 samples\u001b[39;49;00m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Use only the first 50 validation samples\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Use only the first 50 test samples\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclasses_\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 513\u001b[39m, in \u001b[36mtrain_and_evaluate_bert_classifier\u001b[39m\u001b[34m(X_train, y_train, X_val, y_val, X_test, y_test, class_names)\u001b[39m\n\u001b[32m    505\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMacro F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_metrics[\u001b[33m'\u001b[39m\u001b[33mf1_macro\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    506\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMicro F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_metrics[\u001b[33m'\u001b[39m\u001b[33mf1_micro\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    508\u001b[39m results = {\n\u001b[32m    509\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mHamming Loss\u001b[39m\u001b[33m'\u001b[39m: test_metrics[\u001b[33m'\u001b[39m\u001b[33mhamming_loss\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    510\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mHamming Score\u001b[39m\u001b[33m'\u001b[39m: test_metrics[\u001b[33m'\u001b[39m\u001b[33mhamming_score\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    511\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mMacro F1\u001b[39m\u001b[33m'\u001b[39m: test_metrics[\u001b[33m'\u001b[39m\u001b[33mf1_macro\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    512\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mMicro F1\u001b[39m\u001b[33m'\u001b[39m: test_metrics[\u001b[33m'\u001b[39m\u001b[33mf1_micro\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m--> \u001b[39m\u001b[32m513\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mClass Metrics\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mclass_metrics\u001b[49m\n\u001b[32m    514\u001b[39m }\n\u001b[32m    516\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🔹 Test Metrics:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    517\u001b[39m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'class_metrics' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "classifier = train_and_evaluate_bert_classifier(\n",
    "    X_train[:100], y_train[:100],  # Use only the first 100 samples\n",
    "    X_val[:50], y_val[:50],        # Use only the first 50 validation samples\n",
    "    X_test[:50], y_test[:50],      # Use only the first 50 test samples\n",
    "    class_names=mlb.classes_\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = train_and_evaluate_bert_classifier(\n",
    "    X_train, y_train, \n",
    "    X_val, y_val, \n",
    "    X_test, y_test,\n",
    "    class_names=mlb.classes_  # Your MultiLabelBinarizer class names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "if class_names is not None:\n",
    "        class_df.index = class_names\n",
    "    else:\n",
    "        class_df.index = [f'Label {i}' for i in range(num_labels)]\n",
    "    \n",
    "print(\"\\n🔹 Classification Report:\")\n",
    "if class_names is not None:\n",
    "    print(classification_report(y_test, y_pred, target_names=class_names, zero_division=0))\n",
    "else:\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
